---
title: "What is Systematic Evidence Synthesis?"
author: Matthew Grainger
email: matthew.grainger@nina.no
format:
  revealjs:
    slide-number: true
    show-slide-number: all
    logo: images/JUNO_TAGLINE.png
theme: juno-theme.scss
editor: visual
---

## What is Evidence Synthesis?

-   Evidence synthesis is the process of gathering, combining, and interpreting research from multiple studies to draw broader conclusions.

-   We are often tasked with making sense of diverse studies on similar topics. Evidence synthesis allows us to combine results and make decisions based on the bigger picture.

## Why cant we rely on single studies?

::: {.fragment fragment-index="1"}
:::

::: {.fragment fragment-index="1"}
-   Single studies can be misleading or misinterpreted.
-   Single studies can be biased, poorly designed, or limited in scope.
-   They may not represent the full range of conditions.
-   They can be influenced by publication bias
-   They may not be replicable or generalisable to other contexts.
-   They may not be transparent in their methods or data
:::

## Conflicting results

```{r}
#| echo: false
#| warning: false
#| message: false
#| label: conflicts

library(tidyverse)
library(plotly)
set.seed(42)

# Simulating Biodiversity (Species Richness)
biodiversity <- seq(1, 100, by=1)

# Simulating Different Relationships
data <- data.frame(
  biodiversity = biodiversity,
  
  # 1. Positive Relationship (Tilman et al. 1997)
  productivity_positive = biodiversity * 0.5 + rnorm(length(biodiversity), 0, 5),
  
  # 2. Negative Relationship (Garnier et al. 2004)
  productivity_negative = 100 - biodiversity * 0.5 + rnorm(length(biodiversity), 0, 5),
  
  # 3. Hump-Shaped Relationship (Mittelbach et al. 2001)
  productivity_hump = -0.02 * (biodiversity - 50)^2 + 60 + rnorm(length(biodiversity), 0, 5),
  
  # 4. No Relationship (Wardle et al. 2000)
  productivity_random = runif(length(biodiversity), 20, 80)
)

# Transforming Data for Faceted Plot
plot_data <- data |> 
  pivot_longer(cols = starts_with("productivity"), 
               names_to = "Relationship", 
               values_to = "Productivity") |> 
  mutate(Relationship = case_when(
    Relationship == "productivity_positive" ~ "Positive",
    Relationship == "productivity_negative" ~ "Negative",
    Relationship == "productivity_hump" ~ "Hump-Shaped",
    Relationship == "productivity_random" ~ "No Relationship"
  ))

# Adding Example References
references <- data.frame(
  Relationship = c("Positive", "Negative", "Hump-Shaped", "No Relationship"),
  ref_text = c(
    "Tilman et al. 1997 (Science)",  # Positive Relationship
    "Garnier et al. 2004 (Ecology Letters)",  # Negative Relationship
    "Mittelbach et al. 2001 (Ecology)",  # Hump-Shaped Relationship
    "Wardle et al. 2000 (Oikos)"  # No Relationship
  ),
  x_pos = c(20, 40, 50, 50),  # Approximate x-positions for text placement
  y_pos = c(90, 90, 90, 75)  # Approximate y-positions for text placement
)

# Plotting the Relationships with References
ggplot(plot_data, aes(x = biodiversity, y = Productivity)) +
  geom_point(alpha = 0.6, color = "blue") +
  geom_smooth(method = "loess", color = "red", se = FALSE) +
  facet_wrap(~ Relationship, scales = "free") +
  geom_text(data = references, aes(x = x_pos, y = y_pos, label = ref_text), inherit.aes = FALSE, size = 4, hjust = 0) +
  labs(title = "Biodiversity-Productivity Relationships",
       x = "Biodiversity (Species Richness)",
       y = "Productivity") +
  theme_minimal()

```

## Small studies

-   [Small studies have a higher probability of false-positives]{style="color:darkred;"}

```{r}
#| echo: false
#| warning: false
#| message: false
# Load necessary libraries
library(pwr)

# Define sample sizes
sample_sizes <- seq(5, 200, by=5)

# Calculate power for two different effect sizes
effect_sizes <- c(0.5, 0.2)  # Moderate (0.5) and small (0.2) effect sizes

power_data <- data.frame(
  SampleSize = rep(sample_sizes, times = 2),
  Power = c(
    sapply(sample_sizes, function(n) pwr.t.test(n = n, d = 0.5, sig.level = 0.05, type = "two.sample")$power),
    sapply(sample_sizes, function(n) pwr.t.test(n = n, d = 0.2, sig.level = 0.05, type = "two.sample")$power)
  ),
  EffectSize = rep(c("0.5 (Moderate)", "0.2 (Small)"), each = length(sample_sizes))
)

# Plot using ggplot2
ggplot(power_data, aes(x = SampleSize, y = Power, color = EffectSize)) +
  geom_line(size = 1.2) +
  geom_hline(yintercept = 0.8, linetype = "dashed", color = "red") +
  labs(
    title = "Impact of Sample Size on Statistical Power",
    x = "Sample Size",
    y = "Power",
    color = "Effect Size"
  ) +
  theme_minimal() +
  scale_color_manual(values = c("blue", "orange")) +
  theme(legend.position = "bottom")

```

## Effect size - a quick detour...

::::: columns
::: column
![Small effect: Coke vs. Pepsi](images/coke_vs_pepsi.png)

**Small effect**\

*Coke vs. Pepsi: You might notice, but not everyone does.*
:::

::: column
![Large effect: Phone vs. smartphone](images/phones.jpg)

**Large effect**\

*House phone vs. smartphone: Totally obvious and life-changing.*
:::
:::::

## Too many studies

-   [\> 1.5 million papers published each year]{style="color:darkred;"}

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: load_plot_pub_data

data=readRDS("data/publications_data.RDS")
data<-data |> 
   mutate(across(everything(), ~ gsub(",", ".", .))) |> 
  mutate(across(everything(), as.numeric))

p<-data |>
  ggplot(aes(x=year, y=npub_DIM))+ #totpub_DIM)) +
  geom_col(stat="identity", col="darkred") +
  #geom_point(aes(x=year, y=totpub_DIM), color="red") +
  #geom_smooth(method="lm", se=FALSE) +
  labs(title="Number of publications per year",
       x="Year",
       y="Number of publications")+
  theme_classic()
p  


```

# Types of Evidence Synthesis

## Essential steps in evidence synthesis

![](images/clipboard-3014003817.png)

# [The Review family]{style="color:darkred;"}

## Narrative Review

-   A summary of studies, but typically lacks formal methods for study selection or quality assessment.

-   Good for broad overviews but less reliable in terms of conclusions.

-   Example: A narrative review of school feeding programs across low-income countries might describe various initiatives but not assess their quality or impact rigorously.

## Systematic Review

-   Uses strict, [predefined]{style="color\"darkred\""} methods to select studies and assess their quality.

-   Example: *"What is the impact of school feeding programs on child nutrition outcomes?"*

## Meta-Analysis

-   A subset of systematic reviews, but with a statistical approach to combine effect sizes from different studies.

-   Helps estimate the **average effect** of an intervention and whether it's statistically significant.

-   Example: A meta-analysis could calculate the **pooled effect of school meals on test scores** across multiple countries.

[You can have a systematic review without a meta-analysis but not a meta-analysis without a systematic review]{style="color:darkred;"}

## Rapid Review

-   A **streamlined** review using shortcuts (e.g., limiting search databases or skipping quality appraisal) to deliver timely evidence.

-   Ideal for informing **urgent policy** decisions, though potentially less thorough.

-   Example: A rapid review might assess existing evidence on school feeding interventions **during a food crisis or pandemic**.

[There is no accepted definition of a Rapid Review and we do not often know which components we should cut and the impacts of that]{style="color:darkred;"}

## Living Review

-   A **continuously updated** review that incorporates new studies as they become available.

-   Especially useful in **fast-moving areas** where policies and interventions evolve rapidly.

-   Example: A living review on the **impacts of inflation on school meal programs** could update evidence as new economic data or interventions emerge.

## Umbrella Review

-   A **review of systematic reviews**, offering a bird's-eye view of findings across multiple questions or interventions.

-   Helps compare strategies and understand broader trends.

-   Example: An umbrella review could examine multiple systematic reviews on **nutrition interventions in schools**, comparing the effectiveness of feeding, deworming, and education programs.

# [Mapping family]{style="color:darkred;"}

## Systematic Map

-   A mapping process that categorises existing research but does not quantitatively synthesise results.

<!-- -->

-   Identifies **knowledge clusters and gaps**.

-   Example: A systematic map might chart all research on **school feeding interventions in Sub-Saharan Africa**, categorised by outcome (e.g., attendance, nutrition, learning).

## Scoping Review

-   A broader review that identifies the extent of research in a particular field or topic, often used to inform future research agendas.

-   Example: A scoping review could explore the range of outcomes studied in school feeding programs globally-e.g., **health, gender equity, education**.

## Evidence Gap Map (EGM)

-   A **visual tool** to show where evidence exists and where it's lacking.

-   Often used to support **funding and policy** decisions.

-   Example: An EGM on **school feeding and food security** might show many studies on health outcomes but few on long-term economic impacts.

## Miro board

```{r}
library(qrcode) 
plot(qr_code("https://miro.com/welcomeonboard/SDFxNlF0RkcrN2VVOE9Wck9VSXQ2RitNRHl3WUVBbkN5ajhPY3BUbnZZeWp6UzRDQXpWU004amh4YUVUSk1MWmEwZVhGY2pOQUpscnkxOGpCZzZ2WXFYdmR4ZzFqckJMRVZzWjRYT1MyeUdraTRGNjRMU2N5NE44eE82WWhicW1NakdSWkpBejJWRjJhRnhhb1UwcS9BPT0hdjE=?share_link_id=302843227225"))
```

## 

# Why Methodological Rigor Matters

## Different synthesis methods provide varying levels of confidence and transparency

![](images/Pyramid.png)

## 

[![Mupepele, et al. (2016), Ecol Appl, 26: 1295-1301. ](images/Pyramid2.png.jpg)](https://doi.org/10.1890/15-0595)

## Different types of primary evidence

[![Christie, et al. 2019. J Appl Ecol.; 56: 2742–2754. https://doi.org/10.1111/1365-2664.13499](images/clipboard-1547049060.png){width="506"}](https://doi.org/10.1111/1365-2664.13499)

## 

[![Performance of designs](images/clipboard-3628596984.png){width="411"}](https://doi.org/10.1111/1365-2664.14153)

## Common pitfalls

-   Without proper methods to assess study quality and bias, we might miss crucial information that could change the overall interpretation of the evidence

-   Publication bias can skew results, as studies with positive findings are more likely to be published.

-   Small studies can lead to false positives, as they may not have enough power to detect true effects.

## Recommended literature {.smaller}

[Collaboration for Environmental Evidence, 2018. Guidelines and Standards for Evidence synthesis in Environmental Management. Version 5.0](https://environmentalevidence.org/information-for-authors/table-of-contents-page)

[Foo, et al. 2021. A practical guide to question formation, systematic searching and study screening for literature reviews in ecology and evolution. MEE, 12, 1705–1720.](https://doi.org/10.1111/2041-210X.13654)

[Haddaway et al., 2020. Eight problems with literature reviews and how to fix them. NEE, 4(12), pp.1582-1589.](https://doi.org/10.1038/s41559-020-01295-x)

[James et al. 2016. A methodology for systematic mapping in environmental sciences. Env Evi, 5](https://doi.org/10.1186/s13750-016-0059-6)
